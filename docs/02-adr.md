# Architecture Decision Records (ADR)

**Document Version**: v1.0  
**Created Date**: 2026-01-04  
**Last Updated**: 2026-01-26

---

## Overview

This document records key architectural decisions in the Vibe Review project. Each decision includes background, decision content, alternatives, decision rationale, and impact analysis.

---

## ADR-001: Agent Review Protocol Design

### Status
âœ… **Decided** (2026-01-04)

### Context
In the Vibe Coding era, Agent-generated code lacks "author intent" transmission, causing Review to become "reverse engineering." We need a protocol for Agents to provide structured explanations when generating code.

### Decision Content
Adopt **4-layer protocol structure**:
- **WHAT layer**: Intent (intent), Changes (changes)
- **WHY layer**: Rationale (rationale)
- **HOW TO VERIFY layer**: Tests (tests), Edge Cases (edge cases)
- **IMPACT layer**: Impact (impact analysis)

### Alternatives
1. **3-layer structure**: WHAT/WHY/HOW TO VERIFY (without IMPACT)
2. **5-layer structure**: Add META layer as required field
3. **Free format**: No enforced structure, Agents freely express

### Decision Rationale
1. **IMPACT as independent layer**: Impact analysis is important enough to deserve separate attention, especially in complex systems
2. **Structured design**: Ensures completeness and consistency of information transmission
3. **Balanced complexity**: 4-layer structure balances completeness and usability

### Impact
- **Positive Impact**:
  - Provides complete code understanding framework
  - Standardizes Agent output format
  - Facilitates tool parsing and display
- **Negative Impact**:
  - Increases complexity of Agent-generated content
  - Requires Agents to have impact analysis capabilities

### Related Decisions
- Related to ADR-002 (Data Acquisition Strategy): Protocol content is generated through Skill

---

## ADR-002: Data Acquisition Strategy

### Status
âœ… **Decided** (2026-01-04)

### Context
Vibe Review needs to acquire two types of data:
1. **Engineering Data**: Objective data like Git diff, file paths, timestamps
2. **Protocol Data**: Structured explanation content generated by Agent

A single solution cannot satisfy acquisition requirements for both types of data.

### Decision Content
Adopt **Hook + Skill Dual-Track System**:
- **Hook Track**: Passively collect execution chain data through AGENTS framework API
- **Skill Track**: As open protocol standard, let Agents actively generate protocol content
- **Intelligent Fusion**: Automatically handle data conflicts, engineering metrics follow Hook as standard

### Alternatives
1. **Pure Hook Solution**: Only acquire data through Hook, infer protocol content from execution process
2. **Pure Skill Solution**: Only acquire data through Skill, including engineering data
3. **Log Parsing Solution**: Parse Agent output logs to acquire data

### Decision Rationale
1. **Data Accuracy**: Hook-acquired engineering data is more accurate and reliable
2. **Protocol Quality**: Skill-generated protocol content is more structured and complete
3. **Extensibility**: Dual-track system supports differentiated adaptation for different Agents
4. **Fault Tolerance**: When one track fails, the other track can still provide basic functionality

### Impact
- **Positive Impact**:
  - More comprehensive and accurate data acquisition
  - Support for multiple Agent platforms
  - Stronger system fault tolerance
- **Negative Impact**:
  - Increased system complexity
  - Need to handle data conflicts
  - Higher development and maintenance costs

### Related Decisions
- Related to ADR-003 (Skill and Rule Collaboration): Skill is an important component of dual-track system

---

## ADR-003: Skill and Rule Collaboration Approach

### Status
âœ… **Decided** (2026-01-04)

### Context
In the Hook + Skill dual-track system, need to define Skill trigger methods, storage locations, granularity design, and version management strategies.

### Decision Content
- **Rule Location**: Declared in AGENTS.md + independent file defining details
- **Trigger Method**: Agent autonomous invocation (not forced trigger)
- **Skill Granularity**: Core Skills (required) + Optional Skills
- **Version Management**: Rule and Skill synchronous upgrade

### Alternatives
1. **Centralized Management**: All Rules and Skills defined in AGENTS.md
2. **Forced Trigger**: System forces Agent to call Skill
3. **Single Skill**: Only one large, comprehensive Skill
4. **Independent Versioning**: Rule and Skill independent version management

### Decision Rationale
1. **Separation of Concerns**: AGENTS.md responsible for declaration, independent files responsible for implementation details
2. **Agent Autonomy**: Maintains Agent's autonomous decision-making capability, improving adaptability
3. **Progressive Adoption**: Core + Optional design supports progressive feature adoption
4. **Consistency Assurance**: Synchronous version management ensures Rule and Skill consistency

### Impact
- **Positive Impact**:
  - Clear responsibility separation
  - Better Agent compatibility
  - Support for progressive feature extension
- **Negative Impact**:
  - Need to maintain multiple files
  - Complexity of version synchronization

### Related Decisions
- Related to ADR-002 (Data Acquisition Strategy): Skill is implementation method of dual-track system
- Related to ADR-004 (MVP Strategy): Affects MVP implementation priority

---

## ADR-004: MVP Strategy

### Status
âœ… **Decided** (2026-01-04)

### Context
Vibe Review adopts 4-layer architecture (Tool Layer/Data Layer/Product Core Layer/Product Delivery Layer), need to determine MVP implementation strategy and priorities.

### Decision Content
Adopt **Phased Strategy**:
- **Phase 0**: Tool Layer (2-3 weeks) - Data collection and fusion capabilities
- **Phase 1**: Data Layer Validation (2-3 weeks) - CLI tool validates data value
- **Phase 2**: Product Core Layer (4-6 weeks) - Protocol parsing and rendering
- **Phase 3**: Product Delivery Layer (Future) - IDE plugin and integration

### Alternatives
1. **Direct Product Development**: Skip Tool Layer, directly develop product features
2. **Full Stack Parallel**: Develop all four layers simultaneously
3. **Minimum Viable**: Only do basic functionality validation

### Decision Rationale
1. **Risk Control**: Validate data model effectiveness first, avoid building product on wrong foundation
2. **Quick Feedback**: CLI tool can quickly validate core assumptions
3. **Progressive Development**: Each phase has clear validation goals
4. **Resource Optimization**: Avoid investing too many resources in uncertain features

### Impact
- **Positive Impact**:
  - Reduce project risk
  - Quickly get user feedback
  - Progressively validate core assumptions
- **Negative Impact**:
  - Extended product feature delivery time
  - Need additional CLI tool development

### Related Decisions
- Related to all other ADRs: Affects overall implementation strategy and priorities

---

## Decision Change History

### ADR-001 Change Record
- **Initial Version**: 3-layer structure (WHAT/WHY/HOW TO VERIFY)
- **Change Reason**: IMPACT is important enough to deserve separate attention
- **Final Version**: 4-layer structure (+ IMPACT as independent layer)

### ADR-002 Change Record
- **Initial Consideration**: Real-time acquisition vs post-execution analysis
- **Change Reason**: Single solution cannot satisfy all requirements
- **Final Version**: Hook + Skill Dual-Track System

### ADR-003 Change Record
- **Initial Consideration**: Centralized management vs distributed management
- **Change Reason**: Need to balance simplicity and flexibility
- **Final Version**: AGENTS.md + Independent Files

### ADR-004 Change Record
- **Initial Consideration**: Direct product feature development
- **Change Reason**: Need to validate data model effectiveness first
- **Final Version**: Phased Strategy

---

## ADR-005: Open Source Strategy

### Status
âœ… **Decided** (2026-01-26)

### Context
Vibe Review is positioned as an open standard and tool, needing to determine open source scope and license selection to promote community adoption.

### Decision Content
- **License**: MIT
- **Scope**: Full open source (all 4 layers)

| Layer | Description | Open Source |
|-------|-------------|-------------|
| Protocol Specification | Agent Review Protocol v0.3 | âœ… Yes |
| Tool Layer | Collection tools, CLI utilities | âœ… Yes |
| Product Core Layer | Core business logic, data processing | âœ… Yes |
| Product Delivery Layer | VSCode extension, UI components | âœ… Yes |

### Alternatives
1. **Partial Open Source**: Only open source protocol specification and tool layer
2. **Dual License**: Core open source, commercial license for enterprise features
3. **Delayed Decision**: Wait for community growth before deciding

### Decision Rationale
1. **Community Adoption**: MIT license is business-friendly, beneficial for ecosystem building
2. **Transparency**: Full open source allows community to audit and contribute
3. **Ecosystem Growth**: Encourages other Agent products to integrate with the protocol
4. **Early Stage**: Project is in early stage, community building is more important than commercial considerations

### Impact
- **Positive Impact**:
  - Accelerate community adoption
  - Build ecosystem trust
  - Attract more contributors
- **Negative Impact**:
  - Future commercial model may be limited
  - Need to maintain complete codebase publicly

---

## ADR-006: Contributor Detection Mechanism

### Status
âœ… **Decided** (2026-01-26)

### Context
Git author information is unreliable (all commits are from human accounts), need to establish a mechanism to distinguish whether code is AI-generated or human-written based on code content similarity matching.

### Decision Content
Use **Hunk-level similarity matching** to determine code contributor:

**Detection Process**:
```
Step 1: Get Git blame (line-level precision)
        â†’ Get commit info for each line
Step 2: For hunks involved in commit
        â†’ Similarity match with collected Agent generation records
Step 3: Decision logic
        â”œâ”€ Similarity >= 90%  â†’ "AI Generated"
        â”œâ”€ Similarity 70-90% â†’ "AI Generated (Human Modified)"
        â”œâ”€ Similarity < 70%  â†’ "Human Contribution"
        â””â”€ No matching record â†’ "Human Contribution"
```

**Configuration**:
| Decision Item | Choice | Notes |
|---------------|--------|-------|
| Matching Granularity | **Hunk Level** | Aligns with Git diff, balances precision and performance |
| Similarity Algorithm | **Levenshtein Edit Distance** | Simple and reliable, can be upgraded later |
| Threshold Configuration | **Configurable**, set fixed defaults | 90% (Pure AI) / 70% (AI+Modified) |
| Boundary Display | Distinguish "Pure AI" and "AI+Human Modified" | Provide users with clear contributor info |

### Alternatives
1. **Line-Level Matching**: Higher precision but more complex, prone to noise interference
2. **File-Level Matching**: Simpler but too coarse-grained, can't distinguish partial modifications
3. **AST-Based Matching**: Semantic-level matching, higher complexity but more accurate

### Decision Rationale
1. **Git Alignment**: Hunk is the natural unit in Git diff
2. **Performance Balance**: Hunk-level is more efficient than line-level, more precise than file-level
3. **Simplicity**: Levenshtein algorithm is simple and reliable, easy to implement and debug
4. **Iterability**: Threshold configuration supports later adjustments and optimization

### Impact
- **Positive Impact**:
  - Clearly distinguish AI/human code contributions
  - Provide accurate contribution statistics
  - Help Review focus on appropriate content
- **Negative Impact**:
  - Similarity algorithm may have edge case errors
  - Need to store Agent generation records for comparison

---

## ADR-007: MVP Interaction Strategy

### Status
âœ… **Decided** (2026-01-26)

### Context
Different Agent products (Cursor, Claude Code, Windsurf, etc.) have different jump methods, need to determine interaction approach for MVP phase.

### Decision Content
MVP phase adopts **floating window display + command hints** approach:

```
Click on AI-generated code line â†’ Pop up floating window
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ðŸ“ Source: Cursor Session #abc123   â”‚
â”‚ ðŸ“ Conversation: Round #3           â”‚
â”‚ ðŸŽ¯ TODO: Implement login verificationâ”‚
â”‚                                     â”‚
â”‚ ðŸ’¡ Jump Commands:                   â”‚
â”‚ Cursor: Ctrl+K â†’ @history abc...    â”‚
â”‚ Claude: /goto conversation abc...   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Decision Item | Choice | Notes |
|---------------|--------|-------|
| MVP Jump | **Do Not Implement Direct Jump** | Different product jump methods not unified |
| Interaction Method | **Floating Window Display Info + Command Hints** | Provide users with enough info to jump manually |
| Command Template System | **Implement After MVP** | Currently focus on core features |

### Alternatives
1. **Direct Jump Implementation**: Support jump for each Agent product
2. **Only Display Info**: No command hints provided
3. **Plugin System**: Let community implement jump plugins for each Agent

### Decision Rationale
1. **Product Diversity**: Different Agent products have very different jump methods
2. **MVP Focus**: MVP phase should focus on core features (collection + display)
3. **User Empowerment**: Provide sufficient context information, let users decide how to trace

### Impact
- **Positive Impact**:
  - Reduce MVP development complexity
  - Maintain flexibility for different Agent products
- **Negative Impact**:
  - Users need manual operations for jumping
  - Experience is not as smooth as direct jump

---

## ADR-008: Todo Item Definition

### Status
âœ… **Decided** (2026-01-26)

### Context
Need to clarify what "Todo items" mean in the context of Vibe Review system.

### Decision Content
**Confirmed**: Todo items refer to **task breakdown generated by Agent itself**

| Agent | Representation Form |
|-------|---------------------|
| Cursor | Task breakdown list, step planning |
| Claude | Step-by-step execution plan |
| Other Agents | Similar task decomposition output |

**Example**:
When an Agent receives a task like "Implement user login", it may break it down to:
```
1. Create login form component
2. Add form validation logic
3. Implement API call to auth endpoint
4. Handle success/error responses
5. Add session storage
```

These breakdown items are what we call "Todo items".

### Alternatives
1. **User-defined TODO**: TODO comments marked by users in code
2. **Review TODO**: Pending items generated during Review process
3. **Mixed Definition**: Include all of the above

### Decision Rationale
1. **Source Clarity**: Clearly associate with Agent execution context
2. **Traceability**: Can trace back to original conversation and session
3. **Scope Definition**: Avoid confusion with traditional TODO concepts

### Impact
- **Positive Impact**:
  - Clear data model definition
  - Easy to trace Agent execution flow
  - Provide context for Review
- **Negative Impact**:
  - Need to parse Agent's task breakdown output
  - Different Agent output formats need adaptation

---

## Decision Principles

In making the above decisions, we follow these principles:

1. **User Value First**: All decisions focus on improving user experience
2. **Technical Feasibility**: Ensure decisions are technically achievable
3. **Open Standards**: Prioritize open protocols and standards
4. **Progressive Development**: Support progressive feature extension
5. **Controllable Risk**: Control technical and product risks

---

*This document records key architectural decisions of the project, providing decision basis for subsequent development.*
